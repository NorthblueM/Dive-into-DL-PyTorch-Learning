# 《动手学深度学习》PyTorch版学习归纳总结
## 任何模型的基本要素
* 模型
* 数据集
* 损失函数
* 优化函数
* 模型评价

## 模型
- 线性回归模型：单层线性神经网络
- softmax回归模型：分类，将输出值变换成值为正且和为1的概率分布：
- 多层感知机（multilayer perceptron，MLP）：含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。
- 语言模型
- 循环神经网络RNN、GRU、LSTM、深度循环神经网络、双向循环神经网络、注意力机制、Transformer
- 卷积神经网络CNN、卷积层、池化层、LeNet、深度卷积神经网络（AlexNet、使用重复元素的网络（VGG）、⽹络中的⽹络（NiN））、批量归一化（BatchNormalization）、残差网络（ResNet）、稠密连接网络（DenseNet）、TextCNN 模型


## 损失函数
+ 均方误差损失函数
+ 交叉熵损失函数：衡量两个概率分布差异的测量函数

## 优化方法
- 随机梯度下降：小批量随机梯度下降（mini-batch stochastic gradient descent）
- 牛顿法
- Momentum
- AdaGrad
- RMSProp
- AdaDelta
- Adam优化算法：相对之前使用的小批量随机梯度下降，它对学习率相对不那么敏感。


## 模型评价方法
+ 分类准确率
+ 模型精度和计算效率
+ 困惑度（perplexity）:衡量语言模型的好坏，是对交叉熵损失函数做指数运算后得到的值。最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。显然，任何一个有效模型的困惑度必须小于类别个数。困惑度必须小于词典大小vocab_size。
+ 对数均方根误差。实操：将小于1的值设成1，使得取对数时数值更稳定。

## 激活函数
* ReLu
* sigmoid
* tanh  
关于激活函数的选择  
1.ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。  
2.用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。  
3.在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。  
4.在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。

## 一些参数
* 批量大小：batch_size
* 学习率：lr
* 训练周期：epoch
* 权重衰减：weight_decay，正则化的时候的超参。

## 程序流程
1. 生成数据集
2. 读取数据集
3. 初始化模型参数
4. 定义模型
5. 定义损失函数
6. 定义优化函数
7. 训练
8. 模型评价

## 实际情况数据处理流程
1.获取数据集、2.数据预处理、3.模型设计、4.模型验证和模型调整（调参）、4.模型预测以及提交

* 获取和读取数据集：删除无用特征，例如id；
* 预处理数据：
1. 对连续数值的特征做标准化（standardization），对于缺失的特征值，我们将其替换成该特征的均值。标准化后，每个数值特征的均值变为0，所以可以直接用0来替换缺失值。
2. 将离散数值转成指示特征。将缺失值也当作合法的特征值并为其创建指示特征。
3. 通过values属性得到NumPy格式的数据，并转成Tensor方便后面的训练。
* 训练模型：1. 定义损失函数，2. 定义网络模型，3. 定义评价指标，4. 定义训练模型函数
* K折交叉验证：返回第i折交叉验证时所需要的训练和验证数据函数，训练K次并返回训练和验证的平均误差函数，
* 模型选择：改动超参来尽可能减小平均测试误差。有时候会发现一组参数的训练误差可以达到很低，但是在K折交叉验证上的误差可能反而较高。这种现象很可能是由过拟合造成的。因此，当训练误差降低时，我们要观察K折交叉验证上的误差是否也相应降低。
* 定义预测函数。在预测之前，我们会使用完整的训练数据集来重新训练模型，并将预测结果存成提交所需要的格式。

## 模型过拟合、欠拟合与选择
* 训练误差（training error）和泛化误差（generalization error）
* 验证集（validation set）
* K折交叉验证（K-fold cross-validation）：对K次训练误差和验证误差分别求平均
* 欠拟合（underfitting）：模型无法得到较低的训练误差
* 过拟合（overfitting）：模型的训练误差远小于它在测试数据集上的误差。
* 模型复杂度：模型越复杂训练误差逐渐降低，泛化误差存在最小值
* 训练数据集大小：一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。
* 权重衰减：权重衰减等价于L2范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。L2范数正则化令权重w1和w2先自乘小于1的数，再减去不含惩罚项的梯度。因此，L2范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。超参：λ>0。
* 丢弃法：多层感知机，隐藏单元将有p的概率被丢弃掉，有1−p的概率hi会除以1−p做拉伸。丢弃法不改变其输入的期望值。超参：p。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法。丢弃法通过随机丢弃层间元素，使模型不依赖于某一个元素来应对过拟合的

+ 应对模型过拟合：增大训练数据量，权重衰减（Loss函数添加正则项），丢弃法（dropout）
+ 应对欠拟合：提升模型的复杂度。

## 梯度消失、梯度爆炸
* 深度模型数值稳定性的典型问题：消失（vanishing）和爆炸（explosion）  
1. 梯度消失会导致模型训练困难，对参数的优化步长过小，收效甚微，模型收敛十分缓慢  
2. 梯度爆炸会导致模型训练困难，对参数的优化步长过大，难以收敛
* 恒等映射（identity mapping）ϕ(x)=x，即没有激活函数。
* 随机初始化模型参数：如果将每个隐藏单元的参数都初始化为相等的值，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。
* Xavier随机初始化：模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。
* 环境因素：训练没有问题，而应用有问题。  
1. 协变量位移：输入分布P(x)发生了改变，标记函数，即条件分布P(y|x)不会改变，根源是特征分布的变化。  
2. 标签偏移：a. 导致偏移的是标签P(y)上的边缘分布的变化，但类条件分布是不变的P(x|y)时，就会出现相反的问题。b. 当我们认为y导致x时，标签偏移是一个合理的假设。c. 当我们期望标签偏移和协变量偏移保持时，使用来自标签偏移假设的方法通常是有利的。这是因为这些方法倾向于操作看起来像标签的对象，这（在深度学习中）与处理看起来像输入的对象（在深度学习中）相比相对容易一些。d. 标签偏移可以简单理解为测试时出现了训练时没有的标签。
3. 概念偏移：标签本身的定义发生变化的情况。概念偏移可以根据其缓慢变化的特点缓解。
* 如果数据量足够的情况下，确保训练数据集和测试集中的数据取自同一个数据集，可以防止协变量偏移和标签偏移是正确的。如果数据量很少，少到测试集中存在训练集中未包含的标签，就会发生标签偏移。

## 凸优化
* 优化与深度学习
* 优化与估计
- 尽管优化方法可以最小化深度学习中的损失函数值，但本质上优化方法达到的目标与深度学习的目标并不相同。
1. 优化方法目标：训练集损失函数值
2. 深度学习目标：测试集损失函数值（泛化性）
- 优化在深度学习中的挑战： 
1. 局部最小值
2. 鞍点：在一个方向上是极大值，另一个方向是极小值点，一阶导都是为0，但是二阶导有正有负。鞍点和局部极小值相同的是，在该点处的梯度都等于零，不同在于在鞍点附近Hessian矩阵是不定的，非正定，非负定，非半正定(行列式小于0)，而在局部极值附近的Hessian矩阵是正定的。
3. 梯度消失
- 凸性 （Convexity）
- 集合：集合中两点连线，连线部分都在集合内，那就是凸函数。交集具有凸性，并集不一定。
- 函数λf(x)+(1−λ)f(x′)≥f(λx+(1−λ)x′)
- Jensen 不等式：函数值的期望大于期望的函数值
+ 性质：
1. 无局部极小值
2. 与凸集的关系：对于凸函数f(x)，定义集合Sb:={x|x∈X and f(x)≤b}，则集合Sb为凸集
3. 二阶条件：f′′(x)≥0⟺f(x)是凸函数
- 有限制条件的优化问题解决方法：
1. 拉格朗日乘子法
2. 惩罚项
3. 投影


## 梯度下降
* 一维梯度下降
* 证明：沿梯度反方向移动自变量可以减小函数值。泰勒展开
* 学习率、局部极小值是梯度下降算法面临的一个挑战。
+ 多维梯度下降

- 自适应方法：
+ 牛顿法：是通过求解目标函数的一阶导数为0时的参数，进而求出目标函数最小值时的参数。收敛速度很快。海森矩阵的逆在迭代过程中不断减小，可以起到逐步减小步长的效果。
1. 缺点：海森矩阵的逆计算复杂，代价比较大，因此有了拟牛顿法。
2. 优点：牛顿法相比梯度下降的一个优势在于：梯度下降“步幅”的确定比较困难，而牛顿法相当于可以通过Hessian矩阵来调整“步幅”。
3. 证明：在x+ϵ处泰勒展开，最小值点处满足: ∇f(x)=0, 即我们希望∇f(x+ϵ)=0, 对泰勒展开关于ϵ求导，忽略高阶无穷小
4. 收敛性分析：只考虑在函数为凸函数, 且最小值点上f′′(x∗)>0，f″(x∗)>0时的收敛速度。
5. 在牛顿法中，局部极小值也可以通过调整学习率来解决。

+ 预处理 （Heissan阵辅助梯度下降）：x←x−ηdiag(Hf)−1∇x
+ 梯度下降与线性搜索（共轭梯度法）

- 随机梯度下降：
- 随机梯度下降参数更新：对于有n个样本对训练数据集，使用该梯度的一次更新的时间复杂度为O(n)，随机梯度下降更新公式O(1)。
- 动态学习率： piecewise constant、exponential、polynomial。1. 在最开始学习率设计比较大，加速收敛；2. 学习率可以设计为指数衰减或多项式衰减；3. 在优化进行一段时间后可以适当减小学习率来避免振荡。

+ 小批量随机梯度下降
+ 通过train_sgd 函数的参数 “batch_size”来分别使用梯度下降、随机梯度下降和小批量随机梯度下降。

## 优化算法进阶
* 动量法：
* 目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。对于noisy gradient,我们需要谨慎的选取学习率和batch size, 来控制梯度方差和收敛的结果。
* 在二阶优化中，我们使用Hessian matrix的逆矩阵(或者pseudo inverse)来左乘梯度向量i.e.Δx=H−1g，这样的做法称为precondition，相当于将H映射为一个单位矩阵，拥有分布均匀的Spectrum，也即我们去优化的等价标函数的Hessian matrix为良好的identity matrix。

+ RMSProp、Adam、SGD Momentum均使用到Exponential Moving Average，只有Adagrad没有。
+ RMSProp利用Exponential Moving Average解决了Adagrad梯度消失的问题
+ AdaGrad出现梯度消失的原因是自适应学习率分母的不断累加使其存在最终趋于0的可能
+ AdaDelta是基于RMSProp的改进算法，其只有一个超参数。AdaDelta算法没有学习率这一超参数。
+ Adam使用了Momentum算法
+ Adam使用了Exponential Moving Average，Adam对大小相差很大数量级的梯度都可以rescale到相近的大小，Adam是RMSProp和Momentum算法的结合，并对EMA权重进行了无偏操作







## 卷积神经网络
* 二维互相关（cross-correlation）运算：卷积核（kernel）或过滤器（filter）
* 二维卷积层：卷积核、标量偏置
* 互相关运算与卷积运算：核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，所以使用互相关运算与使用卷积运算并无本质区别。
* 特征图（feature map）与感受野（receptive field）：通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。
* 填充（padding）：指在输入高和宽的两侧填充元素（通常是0元素）。在卷积神经网络中使用奇数高宽的核，对于高度（或宽度）为大小为2k+1的核，令步幅为1，在高（或宽）两侧选择大小为k的填充，便可保持输入与输出尺寸相同。
* 步幅（stride）：在互相关运算中，卷积核在输入数组上滑动，每次滑动的行数与列数。
* 当在高和宽上的填充均为p时，我们称填充为p；当在高和宽上的不符均为s时，我们称步幅为s。
* 通道（channel）：多输出通道，一个ci × kh × kw的核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个这样的核数组，不同的核数组提取的是不同的特征。
* 卷积层通过填充、步幅、输入通道数、输出通道数等调节输出的形状。计算feature map 长与宽的公式 = (n+2p-f)/s+1, n 原图片（input）长与宽，p为padding，f是filter的长与宽，s是stride值。
* 1×1卷积核：可在不改变高宽的情况下，调整通道数。1×1卷积核不识别高和宽维度上相邻元素构成的模式，其主要计算发生在通道维上。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1×1卷积层的作用与全连接层等价。
* 二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：
1. 一是全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。
2. 二是卷积层的参数量更少。
* 二维池化层：池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做最大池化或平均池化。
* 池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。
* 在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的输出通道数与输入通道数相等。
* 池化层没有模型参数，池化层通常会减小特征图的高和宽。但参与反向传播。
* 超参：层数、卷积核大小、填充大小、步幅、输出通道数、池化窗口大小、池化步幅、池化填充、池化方式选择。

## LeNet 模型
* 使用全连接层的局限性：
1. 图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。
2. 对于大尺寸的输入图像，使用全连接层容易导致模型过大。
* 使用卷积层的优势：
1. 卷积层保留输入形状。
2. 卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。

* LeNet分为卷积层块和全连接层块两个部分。
1. 卷积层块里的基本单位是卷积层后接平均池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的平均池化层则用来降低卷积层对位置的敏感性。
2. 卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用5×5的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。
3. 全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。
* 总结：卷积神经网络就是含卷积层的网络。 LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。
* LeNet的卷积层块交替使用卷积层和池化层。
* LeNet在连接卷积层块和全连接层块时，需要做一次展平操作。
* 全连接层的参数数量比卷积层多。
* 使用形状为2×2，步幅为2的池化层，会将高和宽都减半。
* 在通过卷积层或池化层后，输出的高和宽可能减小，为了尽可能保留输入的特征，我们可以在减小高宽的同时增加通道数。
* 评价指标：分类准确率；参数初始化：Xavier随机初始化；损失函数：交叉熵损失函数；训练算法：小批量随机梯度下降。

## 深度卷积神经网络
* LeNet: 在大的真实数据集上的表现并不尽如⼈意。
1. 神经网络计算复杂。
2. 还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。

+ 机器学习的特征提取:手工定义的特征提取函数
+ 神经网络的特征提取：通过学习得到数据的多级表征，并逐级表⽰越来越抽象的概念或模式。
+ 神经网络发展的限制:数据、硬件

### AlexNet
* AlexNet：首次证明了学习到的特征可以超越⼿⼯设计的特征，从而⼀举打破计算机视觉研究的前状。
* 特征：
1. 8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。
2. 将sigmoid激活函数改成了更加简单的ReLU激活函数。
3. 用Dropout来控制全连接层的模型复杂度。
4. 引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。

### 使用重复元素的网络（VGG）:
+ VGG：通过重复使⽤简单的基础块来构建深度模型。
+ Block: 数个相同的填充为1、窗口形状为3×3的卷积层,接上一个步幅为2、窗口形状为
2×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。

### ⽹络中的⽹络（NiN）
* LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取 空间特征，再以由全连接层构成的模块来输出分类结果。
* NiN：串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。
* ⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。
* 1×1卷积核作用
1. 放缩通道数：通过控制卷积核的数量达到通道数的放缩。
2. 增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。
3. 计算参数少

* 总结：
- NiN重复使⽤由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层⽹络。
- NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数 的NiN块和全局平均池化层。
- NiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计。

* 文章的新点：
1. 采用 mlpcon 的结构来代替 traditional 卷积层；mlpcon 指的是： multilayer perceptron + convolution;
2. remove 卷积神经网络最后的全连接层，采用 global average pooling 层代替；

### GoogLeNet
* 由Inception基础块组成。 
Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。 
可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 


## 批量归一化（BatchNormalization）
* 对输入的标准化（浅层模型）：
1. 处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。
2. 标准化处理输入数据使各个特征的分布相近

* 批量归一化（深度模型）：
1. 利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。

* 对全连接层做批量归一化：
1. 位置：全连接层中的仿射变换和激活函数之间。
2. 引入可学习参数：拉伸参数γ和偏移参数β。不是超参。

* 对卷积层做批量归⼀化：
1. 位置：卷积计算之后、应⽤激活函数之前。
2. 如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数。 计算：对单通道，batchsize=m,卷积计算输出=pxq 对该通道中m×p×q个元素同时做批量归一化,使用相同的均值和方差。
3. nn.BatchNorm2d(6)的含义是：卷积层的批量归一化，通道数为6

* 预测时的批量归⼀化：
1. 训练：以batch为单位,对每个batch计算均值和方差。
2. 预测：用移动平均估算整个训练数据集的样本均值和方差。

## 残差网络（ResNet）
* 深度学习的问题：深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，准确率也变得更差。
+ 残差块（Residual Block）：
+ 恒等映射：
1. 左边：f(x)=x
2. 右边：f(x)-x=0 （易于捕捉恒等映射的细微波动）
* 在残差块中，输⼊可通过跨层的数据线路更快 地向前传播。
* 残差网络由多个残差块组成。
* 较普通网络而言，残差网络在网络较深时能更好的收敛。

- ResNet模型:
1. 卷积(64,7x7,3)
2. 批量一体化
3. 最大池化(3x3,2)
4. 残差块x4 (通过步幅为2的残差块在每个模块之间减小高和宽)
5. 全局平均池化
6. 全连接

## 稠密连接网络（DenseNet）:
- 主要构建模块：
1. 稠密块（dense block）： 定义了输入和输出是如何连结的。
2. 过渡层（transition layer）：用来控制通道数，使之不过大。1×1卷积层：来减小通道数；步幅为2的平均池化层：减半高和宽
* 在稠密块中，假设由3个输出通道数为8的卷积层组成，稠密块的输入通道数是3，那么稠密块的输出通道数是：计算公式： in_channels + i * out_channels=3*8+3=27


## 数据增强
### 图像增广
* 从深度卷积神经网络的应用来看，大规模数据集是成功应用深度神经网络的前提。
* 图像增广（image augmentation）技术通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。图像增广的另一种解释是，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。例如，可以对图像进行不同方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性。也可以调整亮度、色彩等因素来降低模型对色彩的敏感度。可以说，在当年AlexNet的成功中，图像增广技术功不可没。图像增广技术在计算机视觉里被广泛使用。
* 图像增广的作用：缓解过拟合，增加模型泛化能力，通过旋转/翻转等方式可以降低对某些属性的依赖。

+ 常用的图像增广方法：大部分图像增广方法都有一定的随机性。
+ 翻转和裁剪：左右翻转图像通常不改变物体的类别。它是最早也是最广泛使用的一种图像增广方法。池化层我们解释了池化层能降低卷积层对目标位置的敏感度。除此之外，我们还可以通过对图像随机裁剪来让物体以不同的比例出现在图像的不同位置，这同样能够降低模型对目标位置的敏感性。
+ 变化颜色：可以从4个方面改变图像的颜色：亮度（brightness）、对比度（contrast）、饱和度（saturation）和色调（hue）。
+ 叠加多个图像增广方法：实际应用中我们会将多个图像增广方法叠加使用。
+ 加入噪点

- 使用图像增广训练模型：
- 使用CIFAR-10数据集，而不是之前我们一直使用的Fashion-MNIST数据集。这是因为Fashion-MNIST数据集中物体的位置和尺寸都已经经过归一化处理，而CIFAR-10数据集中物体的颜色和大小区别更加显著。
- 为了在预测时得到确定的结果，我们通常只将图像增广应用在训练样本上，而不在预测时使用含随机操作的图像增广。
- 使用ToTensor将小批量图像转成PyTorch需要的格式，即形状为(批量大小, 通道数, 高, 宽)、值域在0到1之间且类型为32位浮点数。
- 数据集：CIFAR-10，数据增广：左右翻转，模型：ResNet-18；优化算法：Adam；

### 模型微调
* Fashion-MNIST训练数据集：6万张图片
* 学术界当下使用最广泛的大规模图像数据集ImageNet：超过1,000万的图像和1,000类的物体。
* 平常接触到数据集的规模通常在这两者之间。大数据集上得到的复杂模型会过拟合。同时，因为数据量有限，最终训练得到的模型的精度也可能达不到实用的要求。
* 为了应对上述问题，一个显而易见的解决办法是收集更多的数据。虽然目前的数据采集成本已降低了不少，但其成本仍然不可忽略。例如，虽然ImageNet数据集的图像大多跟目标训练集无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别目标也可能同样有效。

+ 迁移学习（transfer learning），将从源数据集学到的知识迁移到目标数据集上。
+ 迁移学习中的一种常用技术：微调（fine tuning）

- 微调由以下4步构成：
1. 在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。
2. 创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。
3. 为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。
4. 在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。
- 当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。

- 导入实验所需的包或模块。torchvision的models包提供了常用的预训练模型。如果希望获取更多的预训练模型，可以使用使用pretrained-models.pytorch仓库。
- 注: 在使用预训练模型时，一定要和预训练时作同样的预处理。 如果你使用的是torchvision的models，那就要求: All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].
- 注: 如果你使用的是其他模型，那可能没有成员变量fc（比如models中的VGG预训练模型），所以正确做法是查看对应模型源码中其定义部分，这样既不会出错也能加深我们对模型的理解。pretrained-models.pytorch仓库貌似统一了接口，但是我还是建议使用时查看一下对应模型的源码。
- 由于是在很大的ImageNet数据集上预训练的，所以参数已经足够好，因此一般只需使用较小的学习率来微调这些参数，而fc中的随机初始化参数一般需要更大的学习率从头训练。PyTorch可以方便的对模型的不同部分设置不同的学习参数，我们在下面代码中将fc的学习率设为已经预训练过的部分的10倍。






## 文本预处理步骤
* 读入文本
* 分词
* 建立字典，将每个词映射到一个唯一的索引（index）。词频统计、清晰低频词汇、去重、构建索引到token的映射、构建token到索引的映射。use_special_token：pad: 证句子同长的补全词, bos: 句子开始, eos: 句子结束, unk: 未登录词
* 将文本从词的序列转换为索引的序列，方便输入模型


## 语言模型
* 作用：一段自然语言文本可以看作是一个离散时间序列，给定一个长度为T的词的序列(w1,w2,…,wT)，语言模型的目标就是评估该序列是否合理，即计算该序列的概率
### 基于统计的语言模型，主要是n元语法（n-gram）
* 词的概率可以通过该词在训练数据集中的相对词频来计算。
* 马尔科夫假设是指一个词的出现只与前面n个词相关，即n阶马尔可夫链（Markov chain of order n）。
* n元语法（n-grams），它是基于n−1阶马尔可夫链的概率语言模型。
* n的取值：当n较小时，n元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当n较大时，n元语法需要计算并存储大量的词频和多词相邻频率。
* 缺陷：1.参数空间过大，2.数据稀疏
* 程序流程：  
1.读取数据集；
2.建立字符索引
3.时序数据的采样（参数：批量大小batch_size和时间步数n）：随机采样和相邻采样（将数据分割n份分别采样，两批间是连续的），

### 基于神经网络的语言模型
### 基于循环神经网络实现语言模型
* 目的：基于当前的输入与过去的输入序列，预测序列的下一个字符。
* one-hot向量
* 采样：每次采样的小批量的形状是（批量大小, 时间步数）。将小批量变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。
* 模型实现  
1.使用困惑度评价模型。  
2.在迭代模型参数前裁剪梯度。  
3.对时序数据采用不同采样方法将导致隐藏状态初始化的不同。采用相邻采样仅在每个训练周期开始的时候初始化隐藏状态是因为相邻的两个批量在原始数据上是连续的。采用随机采样需要在每个小批量更新前初始化隐藏状态。



## 循环神经网络
* 隐藏层Ht能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。由于Ht的计算基于Ht−1，公式的计算是循环的，使用循环计算的网络即循环神经网络（recurrent neural network）。
* 裁剪梯度（clip gradient）：循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。裁剪梯度是一种应对梯度爆炸的方法。假设我们把所有模型参数的梯度拼接成一个向量g，并设裁剪的阈值是θ。裁剪后的梯度min(θ/∥g∥,1)g的L2范数不超过θ。梯度裁剪之后的梯度小于或者等于原梯度。不解决梯度消失。

### GRU
* RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）
* ⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系
+ 重置⻔有助于捕捉时间序列⾥短期的依赖关系；
+ 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。

### LSTM
* 长短期记忆long short-term memory
+ 遗忘门:控制上一时间步的记忆细胞
+ 输入门:控制当前时间步的输入
+ 输出门:控制从记忆细胞到隐藏状态
+ 记忆细胞：⼀种特殊的隐藏状态的信息的流动

### 深度循环神经网络
* Ht(ℓ)=(Ht(ℓ-1), Ht-1(ℓ))

### 双向循环神经网络
* Ht=(Ht→,Ht←)


## 机器翻译
* 机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。 
* 主要特征：输出是单词序列而不是单个单词。 输出序列的长度可能与源序列的长度不同。
* 程序流程：
+ 数据预处理：将数据集清洗、转化为神经网络的输入minbatch。字符在计算机里是以编码的形式存在，我们通常所用的空格是 \x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。而 \xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。再数据预处理的过程中，我们首先需要对数据进行清洗。
+ 分词：字符串---单词组成的列表
+ 建立词典：单词组成的列表---单词id组成的列表
+ 载入数据集
+ Encoder-Decoder：encoder：输入到隐藏状态；decoder：隐藏状态到输出。隐藏状态为语义编码c。可以应用在对话系统、生成式任务中。
+ Sequence to Sequence模型
+ 集束搜索(Beam Search)：简单greedy search；维特比算法：选择整体分数最高的句子（搜索空间太大） 集束搜索；

## 注意力机制
* 对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。
* 在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。
* Attention是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。attention layer得到输出与value的维度一致。对于一个query来说，attention layer会与每一个key计算注意力分数并进行权重的归一化，输出的向量o则是value的加权求和，而每个key计算的权重与value一一对应。
* 不同的attetion layer的区别在于score函数（用于计算query和key的相似性）的选择。
* 两个常用的注意层 Dot-product Attention 和 Multilayer Perceptron Attention。
* Softmax屏蔽：softmax操作符的一个屏蔽操作。masked_softmax。
* 注意力掩码可以用来解决一组变长序列的编码问题。
* 超出2维矩阵的乘法：X和Y是维度分别为(b,n,m)和(b,m,k)的张量，进行b次二维矩阵乘法后得到Z, 维度为(b,n,k)。高维张量的矩阵乘法可用于并行计算多个位置的注意力分数。
* 点积注意力：The dot product 假设query和keys有相同的维度, 通过计算query和key转置的乘积来计算attention score,通常还会除去根号d减少计算出来的score对维度𝑑的依赖性。实现DotProductAttention支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重。不引入新的模型参数。
* 多层感知机注意力：首先将 query and keys 投影到Rh
* 尽管MLPAttention包含一个额外的MLP模型，但如果给定相同的输入和相同的键，我们将获得与DotProductAttention相同的输出。
* 总结：注意力层显式地选择相关的信息。注意层的内存由键-值对组成，因此它的输出接近于键类似于查询的值。

### 引入注意力机制的Seq2seq模型
* attention layer保存着encodering看到的所有信息，即encoding的每一步输出。在decoding阶段，解码器的t时刻的隐藏状态被当作query，encoder的每个时间步的hidden states作为key和value进行attention聚合. Attetion model的输出当作成上下文信息context vector，并与解码器输入Dt拼接起来一起送到解码器。
* 带有注意机制的seq2seq的编码器与Seq2SeqEncoder相同，所以在此处我们只关注解码器。
* 添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态:
1. the encoder outputs of all timesteps：encoder输出的各个状态，被用于attetion layer的memory部分，有相同的key和values
2. the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state
3. the encoder valid length: 编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings）
* 在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的query。然后，将注意力模型的输出与输入嵌入向量连接起来，输入到RNN层。虽然RNN层隐藏状态也包含来自解码器的历史信息，但是attention model的输出显式地选择了enc_valid_len以内的编码器输出，这样attention机制就会尽可能排除其他不相关的信息。

## Transformer模型
* CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。
* RNNs 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。
- Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：
1. Transformer blocks：将seq2seq模型中的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。
2. Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。
3. Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。

+ 多头注意力层：
* 自注意力（self-attention）的结构：自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。自注意力输出了一个与输入长度相同的表征序列，与循环神经网络相比，自注意力对每个元素输出的计算是并行的，所以我们可以高效的实现这个模块。
* 多头注意力层包含h个并行的自注意力层，每一个这种层被成为一个head。对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，这h
个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。
* 题目：在Transformer模型中，注意力头数为h，嵌入向量和隐藏状态维度均为d，那么一个多头注意力层所含的参数量是：参考MultiHeadAttention模块的定义。h个注意力头中，每个的参数量为3d^2，最后的输出层形状为hd∗d，所以参数量共为4hd^2

+ 基于位置的前馈网络：
* Transformer 模块另一个非常重要的部分就是基于位置的前馈网络（FFN），它接受一个形状为（batch_size，seq_length, feature_size）的三维张量。Position-wise FFN由两个全连接层组成，他们作用在最后一维上。因为序列的每个位置的状态都会被单独地更新，所以我们称他为position-wise，这等效于一个1x1的卷积。

+ Add and Norm：
* 除了上面两个模块之外，Transformer还有一个重要的相加归一化层，它可以平滑地整合输入和其他层的输出，因此我们在每个多头注意力层和FFN层后面都添加一个含残差连接的Layer Norm层。这里 Layer Norm 与Batch Norm很相似，唯一的区别在于Batch Norm是对于batch size这个维度进行计算均值和方差的，而Layer Norm则是对最后一维进行计算。层归一化可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能。
* 题目：层归一化有利于加快收敛，减少训练时间成本，层归一化对一个中间层的所有神经元进行归一化，层归一化的效果不会受到batch大小的影响，批归一化（Batch Normalization）才是对每个神经元的输入数据以mini-batch为单位进行汇总

+ 位置编码：
* 与循环神经网络不同，无论是多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新，这种特性帮助我们实现了高效的并行，却丢失了重要的序列顺序的信息。为了更好的捕捉序列信息，Transformer模型引入了位置编码去保持输入序列元素的位置。
* 位置编码是一个二维的矩阵，i对应着序列中的顺序，j对应其embedding vector内部的维度索引。
* positional encoding对于不同维度具有可区分性

+ 编码器：
* 编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层。对于attention模型以及FFN模型，输出维度都是与embedding维度一致的，这也是由于残差连接天生的特性导致的，因为要将前一层的输出与原始输入相加并归一化。
* 整个编码器由n个刚刚定义的Encoder Block堆叠而成，因为残差连接的缘故，中间状态的维度始终与嵌入向量的维度d一致；同时注意到我们把嵌入向量乘以√d以防止其值过小。

+ 解码器：
* Transformer 模型的解码器与编码器结构类似，然而，除了之前介绍的几个模块之外，编码器部分有另一个子模块。该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和层归一化将各个子层的输出相连。
* 仔细来讲，在第t个时间步，当前输入xt是query，那么self attention接受了第t步以及前t-1步的所有输入x1,…,xt−1。在训练时，由于第t位置的输入可以观测到全部的序列，这与预测阶段的情形项矛盾，所以我们要通过将第t个时间步所对应的可观测长度设置为t，以消除不需要看到的未来的信息。
* 对于Transformer解码器来说，构造方式与编码器一样，除了最后一层添加一个dense layer以获得输出的置信度分数。实现Transformer Decoder，除了常规的超参数例如vocab_size embedding_size 之外，解码器还需要编码器的输出 enc_outputs 和句子有效长度 enc_valid_length。

+ 题目：
* A 在训练和预测过程中，解码器部分均只需进行一次前向传播。错误：训练过程1次，预测过程要进行句子长度次。
* B Transformer 内部的注意力模块均为自注意力模块。错误：Decoder 部分的第二个注意力层不是自注意力，key-value来自编码器而query来自解码器。
* C 解码器部分在预测过程中需要使用 Attention Mask。错误：不需要。
* D 自注意力模块理论上可以捕捉任意距离的依赖关系。正确：正确，因为自注意力会计算句子内任意两个位置的注意力权重。



## word2vec
### 词嵌入
* “循环神经网络的从零开始实现”一节中使用 one-hot 向量表示单词，虽然它们构造起来很容易，但通常并不是一个好选择。一个主要的原因是，one-hot 词向量无法准确表达不同词之间的相似度，如我们常常使用的余弦相似度。
* Word2Vec 词嵌入工具的提出正是为了解决上面这个问题，它将每个词表示成一个定长的向量，并通过在语料库上的预训练使得这些向量能较好地表达不同词之间的相似和类比关系，以引入一定的语义信息。
+ 基于两种概率模型的假设，我们可以定义两种 Word2Vec 模型：
1. Skip-Gram 跳字模型：假设背景词由中心词生成，即建模P(wo∣wc)，其中wc为中心词，wo为任一背景词；
2. CBOW (continuous bag-of-words) 连续词袋模型：假设中心词由背景词生成，即建模 P(wc∣Wo)，其中Wo为背景词的集合。

* Word2Vec 能从语料中学到如何将离散的词映射为连续空间中的向量，并保留其语义上的相似关系。

- Skip-Gram 模型的实现：
1. PTB 数据集：需要一个自然语言语料库，模型将从中学习各个单词间的关系。PTB (Penn Tree Bank) 是一个常用的小型语料库，它采样自《华尔街日报》的文章，包括训练集、验证集和测试集。
2. Skip-Gram 跳字模型：在跳字模型中，每个词被表示成两个d维向量，用来计算条件概率。
3. 负采样近似：由于 softmax 运算考虑了背景词可能是词典V中的任一词，对于含几十万或上百万词的较大词典，就可能导致计算的开销过大。我们将以 skip-gram 模型为例，介绍负采样 (negative sampling) 的实现来尝试解决这个问题。注：除负采样方法外，还有层序 softmax (hiererarchical softmax) 方法也可以用来解决计算量过大的问题。
4. 训练模型：（1）损失函数，应用负采样方法后，我们可利用最大似然估计的对数等价形式将损失函数定义，可以直接使用二元交叉熵损失函数进行计算。

* 建立词语索引：
* 二次采样：本数据中一般会出现一些高频词，如英文中的“the”“a”和“in”。通常来说，在一个背景窗口中，一个词（如“chip”）和较低频词（如“microprocessor”）同时出现比和较高频词（如“the”）同时出现对训练词嵌入模型更有益。因此，训练词嵌入模型时可以对词进行二次采样。 具体来说，数据集中每个被索引词wi将有一定概率被丢弃，越高频的词被丢弃的概率越大。
* 提取中心词和背景词：注：数据批量读取的实现需要依赖负采样近似的实现。
* PyTorch 预置的 Embedding 层，PyTorch 预置的批量乘法，Skip-Gram 模型的前向计算

+ 相比于使用 one-hot 向量表示词语，词嵌入模型的优点：（1）训练好的词向量中能够包含更多语义信息（2）词向量的维度是可以自由设定的（3）词嵌入模型需要运用大规模语料进行训练；
+ 在大语料库上进行大规模的词向量训练时，必要的操作：（1）在训练时使用负采样近似，即对每个中心词都采集若干噪音词；（2）分别定义中心词和背景词的词嵌入层；（3）在词典中去掉出现频率极低的词，或将其在文本中替换为'unk'等特殊字符。

### 词嵌入进阶:
* 改进：
* 子词嵌入（subword embedding）：FastText 以固定大小的 n-gram 形式将单词更细致地表示为了子词的集合，而 BPE (byte pair encoding) 算法则能根据语料库的统计信息，自动且动态地生成高频子词的集合；
* GloVe 全局向量的词嵌入: 通过等价转换 Word2Vec 模型的条件概率公式，我们可以得到一个全局的损失函数表达，并在此基础上进一步优化模型。
* 在大规模的语料上训练这些词嵌入模型，并将预训练得到的词向量应用到下游的自然语言处理任务中。

### GloVe 全局向量的词嵌入
* GloVe 官方 提供了多种规格的预训练词向量，语料库分别采用了维基百科、CommonCrawl和推特等，语料库中词语总数也涵盖了从60亿到8,400亿的不同规模，同时还提供了多种词向量维度供下游模型使用。
* torchtext.vocab 中已经支持了 GloVe, FastText, CharNGram 等常用的预训练词向量，我们可以通过声明 torchtext.vocab.GloVe 类的实例来加载预训练好的 GloVe 词向量。
+ 求近义词：由于词向量空间中的余弦相似性可以衡量词语含义的相似性，我们可以通过寻找空间中的 k 近邻，来查询单词的近义词。
+ 求类比词：除了求近义词以外，我们还可以使用预训练词向量求词与词之间的类比关系，例如“man”之于“woman”相当于“son”之于“daughter”。求类比词问题可以定义为：对于类比关系中的4个词“a之于b相当于c于d”，给定前3个词a,b,c求d。求类比词的思路是，搜索与 
vec(c)+vec(b)−vec(a)的结果向量最相似的词向量，其中vec(w)为w的词向量。

- 题目：
- 对于 Skip-Gram, CBOW, GloVe 等词嵌入方法的理解：（1）词嵌入模型的训练本质上是在优化模型预测各词语同时出现的概率；（2）词嵌入模型的设计和训练语料库的选取都很重要；（3）GloVe 模型用到了语料库上全局的统计信息，而 Skip-Gram 和 CBOW 模型则只用到了局部的统计信息；
- 关于 GloVe 方法基于 Skip-Gram 的改动：（1）GloVe 使用了非概率分布的变量，并添加了中心词和背景词的偏差项，这样做是在松弛概率的规范性，即各个概率事件的概率和加起来等于1；（2）GloVe 使用了一个单调递增的权重函数来加权各个损失项；（3）用平方损失函数替代了交叉熵损失函数；（4）GloVe 的损失函数计算公式中用到了语料库上的全局统计信息

## 文本分类
* 文本分类是自然语言处理的一个常见任务，它把一段不定长的文本序列变换为文本的类别。
* 它的一个子问题：使用文本情感分类来分析文本作者的情绪。这个问题也叫情感分析，并有着广泛的应用。
* 同搜索近义词和类比词一样，文本分类也属于词嵌入的下游应用。
* 应用预训练的词向量和含多个隐藏层的双向循环神经网络与卷积神经网络，来判断一段不定长的文本序列中包含的是正面还是负面的情绪。
1. 文本情感分类数据集：斯坦福的IMDb数据集（Stanford’s Large Movie Review Dataset）。
2. 使用循环神经网络进行情感分类
3. 使用卷积神经网络进行情感分类

* （1）预处理数据：读取数据后，我们先根据文本的格式进行单词的切分，再利用 torchtext.vocab.Vocab 创建词典。词典和词语的索引创建好后，就可以将数据集的文本从字符串的形式转换为单词下标序列的形式，以待之后的使用。
* （2）创建数据迭代器：利用 torch.utils.data.TensorDataset，可以创建 PyTorch 格式的数据集，从而创建数据迭代器。
* （3）使用循环神经网络，双向循环神经网络：利用 torch.nn.RNN 或 torch.nn.LSTM 模组，可以很方便地实现双向循环神经网络。
* （4）加载预训练的词向量：由于预训练词向量的词典及词语索引与我们使用的数据集并不相同，所以需要根据目前的词典及索引的顺序来加载预训练词向量。
* （5）训练模型：训练时可以调用之前编写的 train 及 evaluate_accuracy 函数。由于嵌入层的参数是不需要在训练过程中被更新的，所以我们利用 filter 函数和 lambda 表达式来过滤掉模型中不需要更新参数的部分。
* （6）评价模型
* （3）使用卷积神经网络，一维卷积层：（a）. 与二维卷积层一样，一维卷积层使用一维的互相关运算。在一维互相关运算中，卷积窗口从输入数组的最左方开始，按从左往右的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。（b）.多输入通道的一维互相关运算也与多输入通道的二维互相关运算类似：在每个通道上，将核与相应的输入做一维互相关运算，并将通道之间的结果相加得到输出结果；（c）.由二维互相关运算的定义可知，多输入通道的一维互相关运算可以看作单输入通道的二维互相关运算。（d）.时序最大池化层：一维池化层。TextCNN 中使用的时序最大池化（max-over-time pooling）层实际上对应一维全局最大池化层：假设输入包含多个通道，各通道由不同时间步上的数值组成，各通道的输出即该通道所有时间步中最大的数值。因此，时序最大池化层的输入在各个通道上的时间步数可以不同。（e）.为提升计算性能，我们常常将不同长度的时序样本组成一个小批量，并通过在较短序列后附加特殊字符（如0）令批量中各时序样本长度相同。这些人为添加的特殊字符当然是无意义的。由于时序最大池化的主要目的是抓取时序中最重要的特征，它通常能使模型不受人为添加字符的影响。
* (4) TextCNN 模型，除了用一维卷积层替换循环神经网络外，还使用了两个嵌入层，一个的权重固定，另一个则参与训练。

### TextCNN 模型:
+ TextCNN 模型主要使用了一维卷积层和时序最大池化层。假设输入的文本序列由n个词组成，每个词用d维的词向量表示。那么输入样本的宽为n，输入通道数为d。TextCNN 的计算主要分为以下几步。
1. 定义多个一维卷积核，并使用这些卷积核对输入分别做卷积计算。宽度不同的卷积核可能会捕捉到不同个数的相邻词的相关性。
2. 对输出的所有通道分别做时序最大池化，再将这些通道的池化输出值连结为向量。
3. 通过全连接层将连结后的向量变换为有关各类别的输出。这一步可以使用丢弃层应对过拟合。

