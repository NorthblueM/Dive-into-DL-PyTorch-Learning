# 《动手学深度学习》PyTorch版学习归纳总结
## 任何模型的基本要素
* 模型
* 数据集
* 损失函数
* 优化函数
* 模型评价

## 模型
- 线性回归模型：单层线性神经网络
- softmax回归模型：分类，将输出值变换成值为正且和为1的概率分布：
- 多层感知机（multilayer perceptron，MLP）：含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。
- 语言模型
- 循环神经网络RNN、GRU、LSTM、深度循环神经网络、双向循环神经网络、注意力机制、Transformer
- 卷积神经网络CNN、卷积层、池化层、LeNet、深度卷积神经网络（AlexNet、使用重复元素的网络（VGG）、⽹络中的⽹络（NiN））


## 损失函数
+ 均方误差损失函数
+ 交叉熵损失函数：衡量两个概率分布差异的测量函数

## 优化方法
- 随机梯度下降：小批量随机梯度下降（mini-batch stochastic gradient descent）
- Adam优化算法：相对之前使用的小批量随机梯度下降，它对学习率相对不那么敏感。

## 模型评价方法
+ 分类准确率
+ 模型精度和计算效率
+ 困惑度（perplexity）:衡量语言模型的好坏，是对交叉熵损失函数做指数运算后得到的值。最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。显然，任何一个有效模型的困惑度必须小于类别个数。困惑度必须小于词典大小vocab_size。
+ 对数均方根误差。实操：将小于1的值设成1，使得取对数时数值更稳定。

## 激活函数
* ReLu
* sigmoid
* tanh  
关于激活函数的选择  
1.ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。  
2.用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。  
3.在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。  
4.在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。

## 一些参数
* 批量大小：batch_size
* 学习率：lr
* 训练周期：epoch
* 权重衰减：weight_decay，正则化的时候的超参。

## 程序流程
1. 生成数据集
2. 读取数据集
3. 初始化模型参数
4. 定义模型
5. 定义损失函数
6. 定义优化函数
7. 训练
8. 模型评价

## 实际情况数据处理流程
1.获取数据集、2.数据预处理、3.模型设计、4.模型验证和模型调整（调参）、4.模型预测以及提交

* 获取和读取数据集：删除无用特征，例如id；
* 预处理数据：
1. 对连续数值的特征做标准化（standardization），对于缺失的特征值，我们将其替换成该特征的均值。标准化后，每个数值特征的均值变为0，所以可以直接用0来替换缺失值。
2. 将离散数值转成指示特征。将缺失值也当作合法的特征值并为其创建指示特征。
3. 通过values属性得到NumPy格式的数据，并转成Tensor方便后面的训练。
* 训练模型：1. 定义损失函数，2. 定义网络模型，3. 定义评价指标，4. 定义训练模型函数
* K折交叉验证：返回第i折交叉验证时所需要的训练和验证数据函数，训练K次并返回训练和验证的平均误差函数，
* 模型选择：改动超参来尽可能减小平均测试误差。有时候会发现一组参数的训练误差可以达到很低，但是在K折交叉验证上的误差可能反而较高。这种现象很可能是由过拟合造成的。因此，当训练误差降低时，我们要观察K折交叉验证上的误差是否也相应降低。
* 定义预测函数。在预测之前，我们会使用完整的训练数据集来重新训练模型，并将预测结果存成提交所需要的格式。

## 模型过拟合、欠拟合与选择
* 训练误差（training error）和泛化误差（generalization error）
* 验证集（validation set）
* K折交叉验证（K-fold cross-validation）：对K次训练误差和验证误差分别求平均
* 欠拟合（underfitting）：模型无法得到较低的训练误差
* 过拟合（overfitting）：模型的训练误差远小于它在测试数据集上的误差。
* 模型复杂度：模型越复杂训练误差逐渐降低，泛化误差存在最小值
* 训练数据集大小：一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。
* 权重衰减：权重衰减等价于L2范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。L2范数正则化令权重w1和w2先自乘小于1的数，再减去不含惩罚项的梯度。因此，L2范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。超参：λ>0。
* 丢弃法：多层感知机，隐藏单元将有p的概率被丢弃掉，有1−p的概率hi会除以1−p做拉伸。丢弃法不改变其输入的期望值。超参：p。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法。丢弃法通过随机丢弃层间元素，使模型不依赖于某一个元素来应对过拟合的

+ 应对模型过拟合：增大训练数据量，权重衰减（Loss函数添加正则项），丢弃法（dropout）
+ 应对欠拟合：提升模型的复杂度。

## 梯度消失、梯度爆炸
* 深度模型数值稳定性的典型问题：消失（vanishing）和爆炸（explosion）  
1. 梯度消失会导致模型训练困难，对参数的优化步长过小，收效甚微，模型收敛十分缓慢  
2. 梯度爆炸会导致模型训练困难，对参数的优化步长过大，难以收敛
* 恒等映射（identity mapping）ϕ(x)=x，即没有激活函数。
* 随机初始化模型参数：如果将每个隐藏单元的参数都初始化为相等的值，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。
* Xavier随机初始化：模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。
* 环境因素：训练没有问题，而应用有问题。  
1. 协变量位移：输入分布P(x)发生了改变，标记函数，即条件分布P(y|x)不会改变，根源是特征分布的变化。  
2. 标签偏移：a. 导致偏移的是标签P(y)上的边缘分布的变化，但类条件分布是不变的P(x|y)时，就会出现相反的问题。b. 当我们认为y导致x时，标签偏移是一个合理的假设。c. 当我们期望标签偏移和协变量偏移保持时，使用来自标签偏移假设的方法通常是有利的。这是因为这些方法倾向于操作看起来像标签的对象，这（在深度学习中）与处理看起来像输入的对象（在深度学习中）相比相对容易一些。d. 标签偏移可以简单理解为测试时出现了训练时没有的标签。
3. 概念偏移：标签本身的定义发生变化的情况。概念偏移可以根据其缓慢变化的特点缓解。
* 如果数据量足够的情况下，确保训练数据集和测试集中的数据取自同一个数据集，可以防止协变量偏移和标签偏移是正确的。如果数据量很少，少到测试集中存在训练集中未包含的标签，就会发生标签偏移。



## 文本预处理步骤
* 读入文本
* 分词
* 建立字典，将每个词映射到一个唯一的索引（index）。词频统计、清晰低频词汇、去重、构建索引到token的映射、构建token到索引的映射。use_special_token：pad: 证句子同长的补全词, bos: 句子开始, eos: 句子结束, unk: 未登录词
* 将文本从词的序列转换为索引的序列，方便输入模型



## 语言模型
* 作用：一段自然语言文本可以看作是一个离散时间序列，给定一个长度为T的词的序列(w1,w2,…,wT)，语言模型的目标就是评估该序列是否合理，即计算该序列的概率
### 基于统计的语言模型，主要是n元语法（n-gram）
* 词的概率可以通过该词在训练数据集中的相对词频来计算。
* 马尔科夫假设是指一个词的出现只与前面n个词相关，即n阶马尔可夫链（Markov chain of order n）。
* n元语法（n-grams），它是基于n−1阶马尔可夫链的概率语言模型。
* n的取值：当n较小时，n元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当n较大时，n元语法需要计算并存储大量的词频和多词相邻频率。
* 缺陷：1.参数空间过大，2.数据稀疏
* 程序流程：  
1.读取数据集；
2.建立字符索引
3.时序数据的采样（参数：批量大小batch_size和时间步数n）：随机采样和相邻采样（将数据分割n份分别采样，两批间是连续的），

### 基于神经网络的语言模型
### 基于循环神经网络实现语言模型
* 目的：基于当前的输入与过去的输入序列，预测序列的下一个字符。
* one-hot向量
* 采样：每次采样的小批量的形状是（批量大小, 时间步数）。将小批量变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。
* 模型实现  
1.使用困惑度评价模型。  
2.在迭代模型参数前裁剪梯度。  
3.对时序数据采用不同采样方法将导致隐藏状态初始化的不同。采用相邻采样仅在每个训练周期开始的时候初始化隐藏状态是因为相邻的两个批量在原始数据上是连续的。采用随机采样需要在每个小批量更新前初始化隐藏状态。




## 循环神经网络
* 隐藏层Ht能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。由于Ht的计算基于Ht−1，公式的计算是循环的，使用循环计算的网络即循环神经网络（recurrent neural network）。
* 裁剪梯度（clip gradient）：循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。裁剪梯度是一种应对梯度爆炸的方法。假设我们把所有模型参数的梯度拼接成一个向量g，并设裁剪的阈值是θ。裁剪后的梯度min(θ/∥g∥,1)g的L2范数不超过θ。梯度裁剪之后的梯度小于或者等于原梯度。不解决梯度消失。

### GRU
* RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）
* ⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系
+ 重置⻔有助于捕捉时间序列⾥短期的依赖关系；
+ 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。

### LSTM
* 长短期记忆long short-term memory
+ 遗忘门:控制上一时间步的记忆细胞
+ 输入门:控制当前时间步的输入
+ 输出门:控制从记忆细胞到隐藏状态
+ 记忆细胞：⼀种特殊的隐藏状态的信息的流动

### 深度循环神经网络
* Ht(ℓ)=(Ht(ℓ-1), Ht-1(ℓ))

### 双向循环神经网络
* Ht=(Ht→,Ht←)




## 机器翻译
* 机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。 
* 主要特征：输出是单词序列而不是单个单词。 输出序列的长度可能与源序列的长度不同。
* 程序流程：
+ 数据预处理：将数据集清洗、转化为神经网络的输入minbatch。字符在计算机里是以编码的形式存在，我们通常所用的空格是 \x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。而 \xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。再数据预处理的过程中，我们首先需要对数据进行清洗。
+ 分词：字符串---单词组成的列表
+ 建立词典：单词组成的列表---单词id组成的列表
+ 载入数据集
+ Encoder-Decoder：encoder：输入到隐藏状态；decoder：隐藏状态到输出。隐藏状态为语义编码c。可以应用在对话系统、生成式任务中。
+ Sequence to Sequence模型
+ 集束搜索(Beam Search)：简单greedy search；维特比算法：选择整体分数最高的句子（搜索空间太大） 集束搜索；

## 注意力机制
* 对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。
* 在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。
* Attention是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。attention layer得到输出与value的维度一致。对于一个query来说，attention layer会与每一个key计算注意力分数并进行权重的归一化，输出的向量o则是value的加权求和，而每个key计算的权重与value一一对应。
* 不同的attetion layer的区别在于score函数（用于计算query和key的相似性）的选择。
* 两个常用的注意层 Dot-product Attention 和 Multilayer Perceptron Attention。
* Softmax屏蔽：softmax操作符的一个屏蔽操作。masked_softmax。
* 注意力掩码可以用来解决一组变长序列的编码问题。
* 超出2维矩阵的乘法：X和Y是维度分别为(b,n,m)和(b,m,k)的张量，进行b次二维矩阵乘法后得到Z, 维度为(b,n,k)。高维张量的矩阵乘法可用于并行计算多个位置的注意力分数。
* 点积注意力：The dot product 假设query和keys有相同的维度, 通过计算query和key转置的乘积来计算attention score,通常还会除去根号d减少计算出来的score对维度𝑑的依赖性。实现DotProductAttention支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重。不引入新的模型参数。
* 多层感知机注意力：首先将 query and keys 投影到Rh
* 尽管MLPAttention包含一个额外的MLP模型，但如果给定相同的输入和相同的键，我们将获得与DotProductAttention相同的输出。
* 总结：注意力层显式地选择相关的信息。注意层的内存由键-值对组成，因此它的输出接近于键类似于查询的值。

### 引入注意力机制的Seq2seq模型
* attention layer保存着encodering看到的所有信息，即encoding的每一步输出。在decoding阶段，解码器的t时刻的隐藏状态被当作query，encoder的每个时间步的hidden states作为key和value进行attention聚合. Attetion model的输出当作成上下文信息context vector，并与解码器输入Dt拼接起来一起送到解码器。
* 带有注意机制的seq2seq的编码器与Seq2SeqEncoder相同，所以在此处我们只关注解码器。
* 添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态:
1. the encoder outputs of all timesteps：encoder输出的各个状态，被用于attetion layer的memory部分，有相同的key和values
2. the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state
3. the encoder valid length: 编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings）
* 在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的query。然后，将注意力模型的输出与输入嵌入向量连接起来，输入到RNN层。虽然RNN层隐藏状态也包含来自解码器的历史信息，但是attention model的输出显式地选择了enc_valid_len以内的编码器输出，这样attention机制就会尽可能排除其他不相关的信息。

## Transformer
* CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。
* RNNs 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。
- Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：
1. Transformer blocks：将seq2seq模型中的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。
2. Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。
3. Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。

+ 多头注意力层：
* 自注意力（self-attention）的结构：自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。自注意力输出了一个与输入长度相同的表征序列，与循环神经网络相比，自注意力对每个元素输出的计算是并行的，所以我们可以高效的实现这个模块。
* 多头注意力层包含h个并行的自注意力层，每一个这种层被成为一个head。对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，这h
个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。
* 题目：在Transformer模型中，注意力头数为h，嵌入向量和隐藏状态维度均为d，那么一个多头注意力层所含的参数量是：参考MultiHeadAttention模块的定义。h个注意力头中，每个的参数量为3d^2，最后的输出层形状为hd∗d，所以参数量共为4hd^2

+ 基于位置的前馈网络：
* Transformer 模块另一个非常重要的部分就是基于位置的前馈网络（FFN），它接受一个形状为（batch_size，seq_length, feature_size）的三维张量。Position-wise FFN由两个全连接层组成，他们作用在最后一维上。因为序列的每个位置的状态都会被单独地更新，所以我们称他为position-wise，这等效于一个1x1的卷积。

+ Add and Norm：
* 除了上面两个模块之外，Transformer还有一个重要的相加归一化层，它可以平滑地整合输入和其他层的输出，因此我们在每个多头注意力层和FFN层后面都添加一个含残差连接的Layer Norm层。这里 Layer Norm 与Batch Norm很相似，唯一的区别在于Batch Norm是对于batch size这个维度进行计算均值和方差的，而Layer Norm则是对最后一维进行计算。层归一化可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能。
* 题目：层归一化有利于加快收敛，减少训练时间成本，层归一化对一个中间层的所有神经元进行归一化，层归一化的效果不会受到batch大小的影响，批归一化（Batch Normalization）才是对每个神经元的输入数据以mini-batch为单位进行汇总

+ 位置编码：
* 与循环神经网络不同，无论是多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新，这种特性帮助我们实现了高效的并行，却丢失了重要的序列顺序的信息。为了更好的捕捉序列信息，Transformer模型引入了位置编码去保持输入序列元素的位置。
* 位置编码是一个二维的矩阵，i对应着序列中的顺序，j对应其embedding vector内部的维度索引。
* positional encoding对于不同维度具有可区分性

+ 编码器：
* 编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层。对于attention模型以及FFN模型，输出维度都是与embedding维度一致的，这也是由于残差连接天生的特性导致的，因为要将前一层的输出与原始输入相加并归一化。
* 整个编码器由n个刚刚定义的Encoder Block堆叠而成，因为残差连接的缘故，中间状态的维度始终与嵌入向量的维度d一致；同时注意到我们把嵌入向量乘以√d以防止其值过小。

+ 解码器：
* Transformer 模型的解码器与编码器结构类似，然而，除了之前介绍的几个模块之外，编码器部分有另一个子模块。该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和层归一化将各个子层的输出相连。
* 仔细来讲，在第t个时间步，当前输入xt是query，那么self attention接受了第t步以及前t-1步的所有输入x1,…,xt−1。在训练时，由于第t位置的输入可以观测到全部的序列，这与预测阶段的情形项矛盾，所以我们要通过将第t个时间步所对应的可观测长度设置为t，以消除不需要看到的未来的信息。
* 对于Transformer解码器来说，构造方式与编码器一样，除了最后一层添加一个dense layer以获得输出的置信度分数。实现Transformer Decoder，除了常规的超参数例如vocab_size embedding_size 之外，解码器还需要编码器的输出 enc_outputs 和句子有效长度 enc_valid_length。

+ 题目：
* A 在训练和预测过程中，解码器部分均只需进行一次前向传播。错误：训练过程1次，预测过程要进行句子长度次。
* B Transformer 内部的注意力模块均为自注意力模块。错误：Decoder 部分的第二个注意力层不是自注意力，key-value来自编码器而query来自解码器。
* C 解码器部分在预测过程中需要使用 Attention Mask。错误：不需要。
* D 自注意力模块理论上可以捕捉任意距离的依赖关系。正确：正确，因为自注意力会计算句子内任意两个位置的注意力权重。



## 卷积神经网络
* 二维互相关（cross-correlation）运算：卷积核（kernel）或过滤器（filter）
* 二维卷积层：卷积核、标量偏置
* 互相关运算与卷积运算：核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，所以使用互相关运算与使用卷积运算并无本质区别。
* 特征图（feature map）与感受野（receptive field）：通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。
* 填充（padding）：指在输入高和宽的两侧填充元素（通常是0元素）。在卷积神经网络中使用奇数高宽的核，对于高度（或宽度）为大小为2k+1的核，令步幅为1，在高（或宽）两侧选择大小为k的填充，便可保持输入与输出尺寸相同。
* 步幅（stride）：在互相关运算中，卷积核在输入数组上滑动，每次滑动的行数与列数。
* 当在高和宽上的填充均为p时，我们称填充为p；当在高和宽上的不符均为s时，我们称步幅为s。
* 通道（channel）：多输出通道，一个ci × kh × kw的核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个这样的核数组，不同的核数组提取的是不同的特征。
* 卷积层通过填充、步幅、输入通道数、输出通道数等调节输出的形状。计算feature map 长与宽的公式 = (n+2p-f)/s+1, n 原图片（input）长与宽，p为padding，f是filter的长与宽，s是stride值。
* 1×1卷积核：可在不改变高宽的情况下，调整通道数。1×1卷积核不识别高和宽维度上相邻元素构成的模式，其主要计算发生在通道维上。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1×1卷积层的作用与全连接层等价。
* 二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：
1. 一是全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。
2. 二是卷积层的参数量更少。
* 二维池化层：池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做最大池化或平均池化。
* 池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。
* 在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的输出通道数与输入通道数相等。
* 池化层没有模型参数，池化层通常会减小特征图的高和宽。但参与反向传播。
* 超参：层数、卷积核大小、填充大小、步幅、输出通道数、池化窗口大小、池化步幅、池化填充、池化方式选择。

## LeNet 模型
* 使用全连接层的局限性：
1. 图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。
2. 对于大尺寸的输入图像，使用全连接层容易导致模型过大。
* 使用卷积层的优势：
1. 卷积层保留输入形状。
2. 卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。

* LeNet分为卷积层块和全连接层块两个部分。
1. 卷积层块里的基本单位是卷积层后接平均池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的平均池化层则用来降低卷积层对位置的敏感性。
2. 卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用5×5的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。
3. 全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。
* 总结：卷积神经网络就是含卷积层的网络。 LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。
* LeNet的卷积层块交替使用卷积层和池化层。
* LeNet在连接卷积层块和全连接层块时，需要做一次展平操作。
* 全连接层的参数数量比卷积层多。
* 使用形状为2×2，步幅为2的池化层，会将高和宽都减半。
* 在通过卷积层或池化层后，输出的高和宽可能减小，为了尽可能保留输入的特征，我们可以在减小高宽的同时增加通道数。
* 评价指标：分类准确率；参数初始化：Xavier随机初始化；损失函数：交叉熵损失函数；训练算法：小批量随机梯度下降。

## 深度卷积神经网络
* LeNet: 在大的真实数据集上的表现并不尽如⼈意。
1. 神经网络计算复杂。
2. 还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。

+ 机器学习的特征提取:手工定义的特征提取函数
+ 神经网络的特征提取：通过学习得到数据的多级表征，并逐级表⽰越来越抽象的概念或模式。
+ 神经网络发展的限制:数据、硬件

### AlexNet
* AlexNet：首次证明了学习到的特征可以超越⼿⼯设计的特征，从而⼀举打破计算机视觉研究的前状。
* 特征：
1. 8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。
2. 将sigmoid激活函数改成了更加简单的ReLU激活函数。
3. 用Dropout来控制全连接层的模型复杂度。
4. 引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。

### 使用重复元素的网络（VGG）:
+ VGG：通过重复使⽤简单的基础块来构建深度模型。
+ Block: 数个相同的填充为1、窗口形状为3×3的卷积层,接上一个步幅为2、窗口形状为
2×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。

### ⽹络中的⽹络（NiN）
* LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取 空间特征，再以由全连接层构成的模块来输出分类结果。
* NiN：串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。
* ⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。
* 1×1卷积核作用
1. 放缩通道数：通过控制卷积核的数量达到通道数的放缩。
2. 增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。
3. 计算参数少

* 总结：
- NiN重复使⽤由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层⽹络。
- NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数 的NiN块和全局平均池化层。
- NiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计。

* 文章的新点：
1. 采用 mlpcon 的结构来代替 traditional 卷积层；mlpcon 指的是： multilayer perceptron + convolution;
2. remove 卷积神经网络最后的全连接层，采用 global average pooling 层代替；

### GoogLeNet
* 由Inception基础块组成。 
Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。 
可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 











