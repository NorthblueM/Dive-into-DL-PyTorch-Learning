# 《动手学深度学习》PyTorch版学习归纳总结
## 任何模型的基本要素
* 模型
* 数据集
* 损失函数
* 优化函数
* 模型评价

## 模型
- 线性回归模型：单层线性神经网络
- softmax回归模型：分类，将输出值变换成值为正且和为1的概率分布：
- 多层感知机（multilayer perceptron，MLP）：含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。
- 语言模型
- 循环神经网络RNN

## 损失函数
+ 均方误差损失函数
+ 交叉熵损失函数：衡量两个概率分布差异的测量函数

## 优化方法
- 随机梯度下降：小批量随机梯度下降（mini-batch stochastic gradient descent）

## 模型评价方法
+ 分类准确率
+ 模型精度和计算效率
+ 困惑度（perplexity）:衡量语言模型的好坏，是对交叉熵损失函数做指数运算后得到的值。最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。显然，任何一个有效模型的困惑度必须小于类别个数。困惑度必须小于词典大小vocab_size。

## 一些参数
* 批量大小：batch_size
* 学习率：lr
* 训练周期：epoch

## 程序流程
1. 生成数据集
2. 读取数据集
3. 初始化模型参数
4. 定义模型
5. 定义损失函数
6. 定义优化函数
7. 训练
8. 模型评价

## 激活函数
* ReLu
* sigmoid
* tanh  
关于激活函数的选择
ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。
用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。
在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。
在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。

## 文本预处理步骤
* 读入文本
* 分词
* 建立字典，将每个词映射到一个唯一的索引（index）。词频统计、清晰低频词汇、去重、构建索引到token的映射、构建token到索引的映射。use_special_token：pad: 证句子同长的补全词, bos: 句子开始, eos: 句子结束, unk: 未登录词
* 将文本从词的序列转换为索引的序列，方便输入模型

## 语言模型
* 作用：一段自然语言文本可以看作是一个离散时间序列，给定一个长度为T的词的序列(w1,w2,…,wT)，语言模型的目标就是评估该序列是否合理，即计算该序列的概率
### 基于统计的语言模型，主要是n元语法（n-gram）
* 词的概率可以通过该词在训练数据集中的相对词频来计算。
* 马尔科夫假设是指一个词的出现只与前面n个词相关，即n阶马尔可夫链（Markov chain of order n）。
* n元语法（n-grams），它是基于n−1阶马尔可夫链的概率语言模型。
* n的取值：当n较小时，n元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当n较大时，n元语法需要计算并存储大量的词频和多词相邻频率。
* 缺陷：1.参数空间过大，2.数据稀疏
* 程序流程：  
1.读取数据集；
2.建立字符索引
3.时序数据的采样（参数：批量大小batch_size和时间步数n）：随机采样和相邻采样（将数据分割n份分别采样，两批间是连续的），

### 基于神经网络的语言模型
### 基于循环神经网络实现语言模型
* 目的：基于当前的输入与过去的输入序列，预测序列的下一个字符。
* one-hot向量
* 采样：每次采样的小批量的形状是（批量大小, 时间步数）。将小批量变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。
* 模型实现  
1.使用困惑度评价模型。  
2.在迭代模型参数前裁剪梯度。  
3.对时序数据采用不同采样方法将导致隐藏状态初始化的不同。采用相邻采样仅在每个训练周期开始的时候初始化隐藏状态是因为相邻的两个批量在原始数据上是连续的。采用随机采样需要在每个小批量更新前初始化隐藏状态。

## 循环神经网络
* 隐藏层Ht能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。由于Ht的计算基于Ht−1，公式的计算是循环的，使用循环计算的网络即循环神经网络（recurrent neural network）。
* 裁剪梯度（clip gradient）：循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。裁剪梯度是一种应对梯度爆炸的方法。假设我们把所有模型参数的梯度拼接成一个向量g，并设裁剪的阈值是θ。裁剪后的梯度min(θ/∥g∥,1)g的L2范数不超过θ。梯度裁剪之后的梯度小于或者等于原梯度。不解决梯度消失。



